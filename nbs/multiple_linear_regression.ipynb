{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression from Scratch\n",
    "\n",
    "This notebook demonstrates how to implement Multiple Linear Regression from scratch using gradient descent. We'll visualize the data, learning process, and model predictions through interactive plots and animations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Set plot aesthetics\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Dataset path and target column\n",
    "MLR_DATAPATH = 'data/Student_Performance.csv'\n",
    "MLR_YCOL = 'Performance Index'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class MultipleLinearRegression:\n",
    "    def __init__(self, X, y, epochs=1000, lr=0.01):\n",
    "        \"\"\"
",
    "        Initialize the Multiple Linear Regression model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            Feature matrix with shape (n_samples, n_features)\n",
    "        y : array-like\n",
    "            Target vector with shape (n_samples,)\n",
    "        epochs : int, default=1000\n",
    "            Number of training epochs\n",
    "        lr : float, default=0.01\n",
    "            Learning rate for gradient descent\n",
    "        \"\"\"
",
    "        self.X = np.array(X)\n",
    "        self.y = np.array(y)\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.n = self.X.shape[0]\n",
    "        self.n_features = self.X.shape[1]\n",
    "\n",
    "        # Initialize weights (including intercept)\n",
    "        self.beta = np.random.normal(\n",
    "            0, np.sqrt(2 / self.n), self.X.shape[-1] + 1)\n",
    "        \n",
    "        # For tracking learning progress\n",
    "        self.loss_history = []\n",
    "        self.beta_history = []\n",
    "\n",
    "    def forward(self, feat, target):\n",
    "        \"\"\"
",
    "        Update model parameters based on a single training example.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        feat : array-like\n",
    "            Feature vector\n",
    "        target : float\n",
    "            Target value\n",
    "        \"\"\"
",
    "        # Gradient descent update rule\n",
    "        self.beta = self.beta + \\\n",
    "            (self.lr * 2 * (target - self.pred(feat)) * self.beta) / self.n\n",
    "\n",
    "    def pred(self, inp):\n",
    "        \"\"\"
",
    "        Make a prediction for a single input.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        inp : array-like\n",
    "            Input feature vector\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Predicted value\n",
    "        \"\"\"
",
    "        # Add a 1 for the intercept term and compute dot product\n",
    "        return np.dot(np.insert(inp, 0, 1), self.beta)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"
",
    "        Make predictions for multiple inputs.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            Input feature matrix with shape (n_samples, n_features)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        array-like\n",
    "            Predicted values\n",
    "        \"\"\"
",
    "        X = np.array(X)\n",
    "        predictions = []\n",
    "        for i in range(X.shape[0]):\n",
    "            predictions.append(self.pred(X[i]))\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def loss(self):\n",
    "        \"\"\"
",
    "        Calculate mean squared error loss for the entire dataset.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Mean squared error\n",
    "        \"\"\"
",
    "        l = 0\n",
    "        for i in range(self.n):\n",
    "            l += (self.y[i] - self.pred(self.X[i])) ** 2\n",
    "\n",
    "        return l / self.n\n",
    "\n",
    "    def r2_score(self):\n",
    "        \"\"\"
",
    "        Calculate the coefficient of determination (R^2 score).\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            R^2 score\n",
    "        \"\"\"
",
    "        y_pred = self.predict(self.X)\n",
    "        y_mean = np.mean(self.y)\n",
    "        ss_total = np.sum((self.y - y_mean) ** 2)\n",
    "        ss_residual = np.sum((self.y - y_pred) ** 2)\n",
    "        return 1 - (ss_residual / ss_total)\n",
    "\n",
    "    def train(self, verbose=True, save_history=True):\n",
    "        \"\"\"
",
    "        Train the model using gradient descent.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        verbose : bool, default=True\n",
    "            Whether to show progress bar\n",
    "        save_history : bool, default=True\n",
    "            Whether to save loss and parameter history for visualization\n",
    "        \"\"\"
",
    "        # Reset histories\n",
    "        self.loss_history = []\n",
    "        self.beta_history = []\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in (pb := tqdm(range(self.epochs), disable=not verbose)):\n",
    "            # Save current parameters for visualization\n",
    "            if save_history and epoch % 10 == 0:  # Save every 10 epochs to reduce memory usage\n",
    "                self.beta_history.append(self.beta.copy())\n",
    "            \n",
    "            # Iterate through each sample for stochastic gradient descent\n",
    "            for i in range(self.n):\n",
    "                self.forward(self.X[i, :], self.y[i])\n",
    "\n",
    "                # Calculate and store loss\n",
    "                current_loss = self.loss()\n",
    "                if save_history:\n",
    "                    self.loss_history.append(current_loss)\n",
    "                    \n",
    "                # Update progress bar\n",
    "                if verbose:\n",
    "                    pb.set_description(f'Epoch: {epoch} Sample: {i} Loss: {current_loss:.4f}')\n",
    "        \n",
    "        # Calculate final metrics\n",
    "        final_loss = self.loss()\n",
    "        r2 = self.r2_score()\n",
    "        \n",
    "        # Display final model information\n",
    "        print(f'\nTraining complete!')\n",
    "        print(f'Final loss (MSE): {final_loss:.4f}')\n",
    "        print(f'R² score: {r2:.4f}')\n",
    "        \n",
    "        # Print coefficients\n",
    "        print('\nModel parameters:')\n",
    "        print(f'Intercept: {self.beta[0]:.4f}')\n",
    "        for i in range(1, len(self.beta)):\n",
    "            print(f'Beta_{i}: {self.beta[i]:.4f}')\n",
    "            \n",
    "        return self.beta_history, self.loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def plot_correlation_matrix(df, target_col):\n",
    "    \"\"\"
",
    "    Plot correlation matrix for features and target.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        Input dataframe\n",
    "    target_col : str\n",
    "        Name of target column\n",
    "    \"\"\"
",
    "    plt.figure(figsize=(10, 8))\n",
    "    corr = df.corr()\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "    sns.heatmap(corr, mask=mask, annot=True, fmt='.2f', cmap='coolwarm', square=True)\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show correlations with target\n",
    "    target_corr = df.corr()[target_col].sort_values(ascending=False)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    target_corr.drop(target_col).plot(kind='bar')\n",
    "    plt.title(f'Correlation with {target_col}')\n",
    "    plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_pairplot(df, target_col):\n",
    "    \"\"\"
",
    "    Create a pairplot of the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        Input dataframe\n",
    "    target_col : str\n",
    "        Name of target column\n",
    "    \"\"\"
",
    "    if df.shape[1] > 10:  # Too many features for a pairplot\n",
    "        print(\"Too many features for a pairplot. Selecting top 5 correlated features.\")\n",
    "        top_corr = df.drop(target_col, axis=1).corrwith(df[target_col]).abs().sort_values(ascending=False)\n",
    "        top_features = top_corr.head(5).index.tolist()\n",
    "        features_to_plot = top_features + [target_col]\n",
    "        sns.pairplot(df[features_to_plot], height=2.5, corner=True)\n",
    "    else:\n",
    "        sns.pairplot(df, height=2.5, corner=True)\n",
    "    plt.suptitle('Pairwise Relationships', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_feature_importances_plot(model, feature_names):\n",
    "    \"\"\"
",
    "    Create a bar plot of feature importances.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : MultipleLinearRegression\n",
    "        Trained model\n",
    "    feature_names : list\n",
    "        List of feature names\n",
    "    \"\"\"
",
    "    # Get coefficients (skip intercept)\n",
    "    coefficients = model.beta[1:]\n",
    "    \n",
    "    # Create a DataFrame with feature names and their coefficients\n",
    "    coef_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
    "    \n",
    "    # Sort by absolute coefficient value\n",
    "    coef_df['AbsCoefficient'] = np.abs(coef_df['Coefficient'])\n",
    "    coef_df = coef_df.sort_values('AbsCoefficient', ascending=False)\n",
    "    \n",
    "    # Create bar plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bars = plt.barh(coef_df['Feature'], coef_df['Coefficient'])\n",
    "    \n",
    "    # Color positive and negative coefficients differently\n",
    "    for i, bar in enumerate(bars):\n",
    "        if coef_df['Coefficient'].iloc[i] < 0:\n",
    "            bar.set_color('salmon')\n",
    "        else:\n",
    "            bar.set_color('skyblue')\n",
    "    \n",
    "    plt.title('Feature Importance (Model Coefficients)')\n",
    "    plt.axvline(x=0, color='gray', linestyle='--')\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_learning_curve(loss_history):\n",
    "    \"\"\"
",
    "    Plot the learning curve from training history.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    loss_history : list\n",
    "        List of loss values during training\n",
    "    \"\"\"
",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(loss_history)\n",
    "    plt.title('Learning Curve')\n",
    "    plt.xlabel('Update Step')\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "    plt.yscale('log')  # Log scale often better shows progress\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_weight_trajectory_animation(beta_history, feature_names):\n",
    "    \"\"\"
",
    "    Create an animation showing the trajectory of weights during training.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    beta_history : list\n",
    "        List of beta values at different points during training\n",
    "    feature_names : list\n",
    "        List of feature names\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    HTML\n",
    "        Animation in HTML format\n",
    "    \"\"\"
",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Prepare data\n",
    "    all_weights = np.array(beta_history)\n",
    "    weight_names = ['Intercept'] + feature_names\n",
    "    \n",
    "    # Set up plot\n",
    "    def update(frame):\n",
    "        ax.clear()\n",
    "        weights = all_weights[frame]\n",
    "        bars = ax.barh(weight_names, weights)\n",
    "        \n",
    "        # Color positive and negative weights differently\n",
    "        for i, bar in enumerate(bars):\n",
    "            if weights[i] < 0:\n",
    "                bar.set_color('salmon')\n",
    "            else:\n",
    "                bar.set_color('skyblue')\n",
    "                \n",
    "        ax.set_title(f'Weight Evolution (Step {frame*10})')\n",
    "        ax.axvline(x=0, color='gray', linestyle='--')\n",
    "        ax.set_xlim(all_weights.min() - 0.1, all_weights.max() + 0.1)\n",
    "    \n",
    "    # Create animation\n",
    "    anim = FuncAnimation(fig, update, frames=len(beta_history), interval=100)\n",
    "    plt.close()  # Prevent displaying static plot\n",
    "    return HTML(anim.to_jshtml())\n",
    "\n",
    "def plot_residuals(model, X, y):\n",
    "    \"\"\"
",
    "    Plot residuals analysis for model diagnostics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : MultipleLinearRegression\n",
    "        Trained model\n",
    "    X : array-like\n",
    "        Input features\n",
    "    y : array-like\n",
    "        Target values\n",
    "    \"\"\"
",
    "    # Make predictions\n",
    "    y_pred = model.predict(X)\n",
    "    residuals = y - y_pred\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # 1. Residuals vs Predicted\n",
    "    axes[0].scatter(y_pred, residuals, alpha=0.6)\n",
    "    axes[0].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[0].set_xlabel('Predicted Values')\n",
    "    axes[0].set_ylabel('Residuals')\n",
    "    axes[0].set_title('Residuals vs Predicted Values')\n",
    "    \n",
    "    # 2. Residuals Distribution\n",
    "    sns.histplot(residuals, kde=True, ax=axes[1])\n",
    "    axes[1].axvline(x=0, color='r', linestyle='--')\n",
    "    axes[1].set_xlabel('Residual Value')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title('Residuals Distribution')\n",
    "    \n",
    "    # 3. Q-Q Plot for normality check\n",
    "    from scipy import stats\n",
    "    stats.probplot(residuals, plot=axes[2])\n",
    "    axes[2].set_title('Q-Q Plot (Normality Check)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "try:\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(MLR_DATAPATH)\n",
    "    print(f\"Dataset loaded with {df.shape[0]} samples and {df.shape[1]} features\")\n",
    "    \n",
    "    # Display first few rows\n",
    "    print(f\"\nFirst 5 samples:\n{df.head()}\")\n",
    "    \n",
    "    # Show basic statistics\n",
    "    print(f\"\nDataset info:\")\n",
    "    display(df.describe())\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    if missing_values.sum() > 0:\n",
    "        print(f\"\nMissing values:\n{missing_values[missing_values > 0]}\")\n",
    "    else:\n",
    "        print(\"\nNo missing values found.\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Dataset not found at {MLR_DATAPATH}\")\n",
    "    print(\"Creating synthetic data for demonstration...\")\n",
    "    \n",
    "    # Create synthetic data\n",
    "    np.random.seed(42)\n",
    "    n_samples = 200\n",
    "    n_features = 4\n",
    "    \n",
    "    # Create feature names for synthetic data\n",
    "    feature_names = [f'Feature_{i+1}' for i in range(n_features)]\n",
    "    \n",
    "    # Generate random features\n",
    "    X = np.random.normal(0, 1, (n_samples, n_features))\n",
    "    \n",
    "    # Generate target with noise\n",
    "    true_coef = np.array([3.5, -2.0, 1.5, 0.5])  # True coefficients\n",
    "    intercept = 5.0\n",
    "    y = intercept + np.dot(X, true_coef) + np.random.normal(0, 1, n_samples)\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    data = np.column_stack([X, y])\n",
    "    df = pd.DataFrame(data, columns=feature_names + [MLR_YCOL])\n",
    "    \n",
    "    # Display synthetic data information\n",
    "    print(f\"Synthetic dataset created with {n_samples} samples and {n_features} features\")\n",
    "    print(f\"True coefficients: {true_coef}\")\n",
    "    print(f\"True intercept: {intercept}\")\n",
    "    print(f\"\nFirst 5 samples:\n{df.head()}\")\n",
    "    \n",
    "# Enable this code if you want to test with sklearn's datasets\n",
    "# from sklearn.datasets import fetch_california_housing\n",
    "# housing = fetch_california_housing()\n",
    "# df = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "# df[MLR_YCOL] = housing.target\n",
    "# print(f\"California Housing dataset loaded with {df.shape[0]} samples and {df.shape[1]-1} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize the correlation matrix\n",
    "plot_correlation_matrix(df, MLR_YCOL)\n",
    "\n",
    "# Create pairplot for exploring relationships\n",
    "plot_pairplot(df, MLR_YCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Split features and target\n",
    "X = df.drop(MLR_YCOL, axis=1).values\n",
    "y = df[MLR_YCOL].values\n",
    "feature_names = df.drop(MLR_YCOL, axis=1).columns.tolist()\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Scale the data (important for gradient descent)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Data prepared for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Multiple Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize and train the model\n",
    "mlr = MultipleLinearRegression(X_train_scaled, y_train, epochs=100, lr=0.01)\n",
    "beta_history, loss_history = mlr.train(verbose=True)\n",
    "\n",
    "# Plot learning curve\n",
    "plot_learning_curve(loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Weight Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create and display weight evolution animation\n",
    "weight_animation = create_weight_trajectory_animation(beta_history, feature_names)\n",
    "weight_animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize feature importances\n",
    "create_feature_importances_plot(mlr, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Make predictions on test set\n",
    "y_pred_train = mlr.predict(X_train_scaled)\n",
    "y_pred_test = mlr.predict(X_test_scaled)\n",
    "\n",
    "# Calculate performance metrics\n",
    "train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Model Performance:\n\")\n",
    "print(f\"Training set MSE: {train_mse:.4f}\")\n",
    "print(f\"Test set MSE: {test_mse:.4f}\n\")\n",
    "print(f\"Training set R²: {train_r2:.4f}\")\n",
    "print(f\"Test set R²: {test_r2:.4f}\")\n",
    "\n",
    "# Plot true vs predicted values\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Training set\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_train, y_pred_train, alpha=0.5)\n",
    "plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--')\n",
    "plt.title(f'Training Set: True vs Predicted\nR² = {train_r2:.4f}')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "\n",
    "# Test set\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(y_test, y_pred_test, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.title(f'Test Set: True vs Predicted\nR² = {test_r2:.4f}')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot residuals analysis\n",
    "plot_residuals(mlr, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with Statsmodels Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Add constant for intercept\n",
    "X_train_sm = sm.add_constant(X_train_scaled)\n",
    "X_test_sm = sm.add_constant(X_test_scaled)\n",
    "\n",
    "# Fit the statsmodels OLS model\n",
    "sm_model = sm.OLS(y_train, X_train_sm).fit()\n",
    "\n",
    "# Display the summary\n",
    "print(sm_model.summary())\n",
    "\n",
    "# Compare coefficients\n",
    "print(\"\nCoefficient Comparison:\n\")\n",
    "print(f\"{'Parameter':<15} {'Our Model':<15} {'Statsmodels':<15}\")\n",
    "print(\"-\" * 45)\n",
    "print(f\"{'Intercept':<15} {mlr.beta[0]:<15.4f} {sm_model.params[0]:<15.4f}\")\n",
    "\n",
    "for i, feature in enumerate(feature_names):\n",
    "    print(f\"{feature:<15} {mlr.beta[i+1]:<15.4f} {sm_model.params[i+1]:<15.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D Visualization for Two-Feature Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# This visualization only works well if we have exactly 2 features\n",
    "if X.shape[1] == 2:\n",
    "    # Create a meshgrid for the 3D surface\n",
    "    x_min, x_max = X_train_scaled[:, 0].min() - 1, X_train_scaled[:, 0].max() + 1\n",
    "    y_min, y_max = X_train_scaled[:, 1].min() - 1, X_train_scaled[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 50),\n",
    "                         np.linspace(y_min, y_max, 50))\n",
    "    \n",
    "    # Make predictions on the meshgrid\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = mlr.predict(grid).reshape(xx.shape)\n",
    "    \n",
    "    # Create 3D plot\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Plot the data points\n",
    "    ax.scatter(X_train_scaled[:, 0], X_train_scaled[:, 1], y_train, \n",
    "              c='blue', marker='o', alpha=0.5, label='Training data')\n",
    "    \n",
    "    # Plot the regression surface\n",
    "    surf = ax.plot_surface(xx, yy, Z, cmap='viridis', alpha=0.7, \n",
    "                          linewidth=0, antialiased=True, label='Regression plane')\n",
    "    \n",
    "    # Add colorbar\n",
    "    fig.colorbar(surf, ax=ax, shrink=0.5, aspect=5)\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xlabel(feature_names[0])\n",
    "    ax.set_ylabel(feature_names[1])\n",
    "    ax.set_zlabel(MLR_YCOL)\n",
    "    ax.set_title('Multiple Linear Regression: Prediction Surface')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"3D visualization is only available for datasets with exactly 2 features.\")\n",
    "    \n",
    "    # If more than 2 features, we'll create a partial dependence plot for the two most important features\n",
    "    if X.shape[1] > 2:\n",
    "        print(\"Creating partial dependence plots for the two most important features instead.\")\n",
    "        \n",
    "        # Get the two most important features based on coefficient magnitude\n",
    "        feature_importance = np.abs(mlr.beta[1:])\n",
    "        top_indices = np.argsort(feature_importance)[-2:]\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        \n",
    "        # For each of the top two features\n",
    "        for i, feature_idx in enumerate(top_indices):\n",
    "            # Create a range of values for this feature\n",
    "            feature_values = np.linspace(\n",
    "                X_train_scaled[:, feature_idx].min(), \n",
    "                X_train_scaled[:, feature_idx].max(), \n",
    "                100\n",
    "            )\n",
    "            \n",
    "            # Create predictions where only this feature varies\n",
    "            predictions = []\n",
    "            \n",
    "            # Use the mean values for all other features\n",
    "            X_mean = np.mean(X_train_scaled, axis=0)\n",
    "            \n",
    "            for value in feature_values:\n",
    "                X_sample = X_mean.copy()\n",
    "                X_sample[feature_idx] = value\n",
    "                predictions.append(mlr.pred(X_sample))\n",
    "            \n",
    "            # Plot the partial dependence\n",
    "            axes[i].plot(feature_values, predictions)\n",
    "            axes[i].set_xlabel(feature_names[feature_idx])\n",
    "            axes[i].set_ylabel(f'Predicted {MLR_YCOL}')\n",
    "            axes[i].set_title(f'Partial Dependence Plot for {feature_names[feature_idx]}')\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions with the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example of using the model for predictions\n",
    "def make_prediction(input_data, feature_names, scaler, model):\n",
    "    \"\"\"
",
    "    Make a prediction using the trained model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_data : dict\n",
    "        Dictionary with feature names as keys and values as inputs\n",
    "    feature_names : list\n",
    "        List of feature names\n",
    "    scaler : StandardScaler\n",
    "        Fitted scaler for transforming inputs\n",
    "    model : MultipleLinearRegression\n",
    "        Trained model\n",
    "    \"\"\"
",
    "    # Convert input_data to a numpy array in the correct order\n",
    "    input_array = np.array([input_data[feature] for feature in feature_names]).reshape(1, -1)\n",
    "    \n",
    "    # Scale the input\n",
    "    input_scaled = scaler.transform(input_array)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.pred(input_scaled[0])\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# Choose some example values from the test set\n",
    "example_inputs = {}\n",
    "for i, feature in enumerate(feature_names):\n",
    "    # Use mean values for demonstration\n",
    "    example_inputs[feature] = X_test[0, i]\n",
    "\n",
    "# Make prediction\n",
    "predicted_value = make_prediction(example_inputs, feature_names, scaler, mlr)\n",
    "actual_value = y_test[0]\n",
    "\n",
    "print(f\"Prediction Example:\")\n",
    "print(f\"Input values:\")\n",
    "for feature, value in example_inputs.items():\n",
    "    print(f\"  {feature}: {value:.4f}\")\n",
    "\n",
    "print(f\"\nPredicted {MLR_YCOL}: {predicted_value:.4f}\")\n",
    "print(f\"Actual {MLR_YCOL}: {actual_value:.4f}\")\n",
    "print(f\"Absolute error: {abs(predicted_value - actual_value):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we've implemented Multiple Linear Regression from scratch using gradient descent. We visualized the data, learning process, and model predictions through interactive plots and animations.\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "1. Multiple Linear Regression extends Simple Linear Regression to handle multiple input features\n",
    "2. Gradient descent iteratively updates the model parameters to minimize the loss function\n",
    "3. Feature scaling is crucial for efficient gradient descent convergence\n",
    "4. Visualizations help understand the relationships between features and the target variable\n",
    "5. Residual analysis is important for validating model assumptions\n",
    "6. Our implementation achieves comparable results to the statsmodels implementation\n",
    "\n",
    "The model can be used to predict future values based on new feature inputs, as demonstrated in the prediction example.\n",
    "\n",
    "Further improvements could include:\n",
    "- Implementing regularization (L1/L2) to prevent overfitting\n",
    "- Adding feature selection methods\n",
    "- Implementing different optimization techniques (e.g., stochastic gradient descent with momentum)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

